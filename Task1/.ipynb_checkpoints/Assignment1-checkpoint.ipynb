{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8507de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755826e",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d2125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegr:\n",
    "    def fit(self, X, Y):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (n, m)\n",
    "        #  Y = np.array, shape = (n)\n",
    "        # Finds theta minimising quadratic loss function L, using an explicit formula.\n",
    "        # Note: before applying the formula to X one should append to X a column with ones.\n",
    "        n, m = X.shape\n",
    "        X = np.concatenate((np.ones((n, 1), dtype=int), X), axis=1)\n",
    "        X_T = X.T\n",
    "        self.theta = np.linalg.inv(X_T @ X) @ X_T @ Y\n",
    "        #y_pred = X @ self.theta\n",
    "        #L = sum((Y - y_pred)**2)\n",
    "        #S = sum((Y - np.mean(Y))**2)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        k, m = X.shape\n",
    "        if k > 2 or m > 2:\n",
    "            intercept = self.theta[0]\n",
    "            coef = self.theta[1:]\n",
    "            return intercept + X @ coef\n",
    "        return self.theta[1]*X + self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7fa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,2,10]).reshape((3,1))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    assert list(actual) == pytest.approx(list(expected))\n",
    "\n",
    "def test_RegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    a = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    expected = LinearRegression().fit(X, Y).predict(a)\n",
    "    actual = LinearRegr().fit(X, Y).predict(a)\n",
    "    assert list(actual) == pytest.approx(list(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ed7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RegressionInOneDim()\n",
    "test_RegressionInThreeDim()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07c50a",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad55f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 10**5\n",
    "LEARNING_RATE = 10**(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac81a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegr:\n",
    "    def __init__(self, alpha = 0.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def basic_fit(self, X, Y):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (n, m)\n",
    "        #  Y = np.array, shape = (n)\n",
    "        # Finds theta (approximately) minimising quadratic loss function L with Ridge penalty,\n",
    "        # using an iterative method.\n",
    "        \n",
    "        \n",
    "        n, m = X.shape\n",
    "        X = np.concatenate((np.ones((n, 1), dtype=int), X), axis=1)\n",
    "        X_T = X.T\n",
    "        I = np.identity(m+1)\n",
    "        I[0][0] = 0\n",
    "        self.theta = np.linalg.inv(X_T @ X + self.alpha*I) @ X_T @ Y\n",
    "        return self\n",
    "    \n",
    "    def ridgeGradient(self, X, Y, predictions):\n",
    "        grad = -(2/Y.size) * X.T.dot((Y - predictions)) + self.alpha * self.theta\n",
    "        #grad[0] = - 2 * np.sum(Y - predictions) / Y.size\n",
    "        return grad\n",
    "    \n",
    "    def fit(self, X, Y, num_of_iterations=ITERATIONS, learning_rate=LEARNING_RATE):\n",
    "        n, m = X.shape\n",
    "        X = np.concatenate((np.ones((n, 1), dtype=int), X), axis=1)\n",
    "        X_T = X.T\n",
    "        self.theta = np.zeros(m+1)\n",
    "        #self.theta[0] = np.random.normal()\n",
    "        for i in range(num_of_iterations):\n",
    "            pred = X @ self.theta\n",
    "            grad = self.ridgeGradient(X, Y, pred)\n",
    "            self.theta -= learning_rate*grad \n",
    "        print(self.theta)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, X, Y, num_of_iterations=ITERATIONS, learning_rate=LEARNING_RATE):\n",
    "        n, m = X.shape\n",
    "        #X = np.concatenate((np.ones((n, 1), dtype=int), X), axis=1)\n",
    "        #X_T = X.T\n",
    "        #self.theta = np.zeros(m+1)\n",
    "        #self.theta[0] = np.random.normal()\n",
    "        #for i in range(num_of_iterations):\n",
    "        #    pred = X @ self.theta\n",
    "        #    for i in range(len(self.theta)):\n",
    "        #        if i==0:\n",
    "        #            derivative = 2 * np.dot(Y-pred, X[:, i])\n",
    "        #        else:\n",
    "        #            derivative = 2 * np.dot(Y-pred, X[:, i]) + 2*(self.alpha*self.theta)\n",
    "        #        #grad = self.ridgeGradient(X, Y, pred)\n",
    "        #        self.theta -= learning_rate*derivative\n",
    "        #print(self.theta)\n",
    "        #return self\n",
    "        #self.theta = ridge_regression_gradient_descent(X, Y, np.zeros(m), LEARNING_RATE, 1e11, max_iterations=ITERATIONS)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # input:\n",
    "        #  X = np.array, shape = (k, m)\n",
    "        # returns:\n",
    "        #  Y = wektor(f(X_1), ..., f(X_k))\n",
    "        k, m = X.shape\n",
    "        if k > 2 or m > 2:\n",
    "            #intercept = self.theta[0]\n",
    "            #coef = self.theta[1:]\n",
    "            return  X @ self.theta[1:]\n",
    "        return self.theta[1]*X# + self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62770b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RidgeRegressionInOneDim():\n",
    "    X = np.array([1,3,2,5]).reshape((4,1))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,2,10]).reshape((3,1))\n",
    "    alpha = 0.3\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    print(Ridge(alpha).fit(X, Y).coef_)\n",
    "    print(expected)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(actual)\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-5)\n",
    "\n",
    "def test_RidgeRegressionInThreeDim():\n",
    "    X = np.array([1,2,3,5,4,5,4,3,3,3,2,5]).reshape((4,3))\n",
    "    Y = np.array([2,5, 3, 8])\n",
    "    X_test = np.array([1,0,0, 0,1,0, 0,0,1, 2,5,7, -2,0,3]).reshape((5,3))\n",
    "    alpha = 0.4\n",
    "    expected = Ridge(alpha).fit(X, Y).predict(X_test)\n",
    "    print(Ridge(alpha).fit(X, Y).coef_)\n",
    "    print(expected)\n",
    "    actual = RidgeRegr(alpha).fit(X, Y).predict(X_test)\n",
    "    print(actual)\n",
    "    assert list(actual) == pytest.approx(list(expected), rel=1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e56bb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.49171271]\n",
      "[ 1.88950276  3.38121547 15.31491713]\n",
      "[0.37289248 1.48681635]\n",
      "[ 1.48681635  2.9736327  14.86816352]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_RidgeRegressionInOneDim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#test_RidgeRegressionInThreeDim()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mtest_RidgeRegressionInOneDim\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m actual \u001b[38;5;241m=\u001b[39m RidgeRegr(alpha)\u001b[38;5;241m.\u001b[39mfit(X, Y)\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(actual)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(actual) \u001b[38;5;241m==\u001b[39m pytest\u001b[38;5;241m.\u001b[39mapprox(\u001b[38;5;28mlist\u001b[39m(expected), rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_RidgeRegressionInOneDim()\n",
    "#test_RidgeRegressionInThreeDim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88580b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
